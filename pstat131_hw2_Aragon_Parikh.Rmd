---
title: "PSTAT 131 Homework 2"
author: "Luis Aragon and Ronak Parikh"
date: "5/1/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Set Up
```{r}
library(tidyverse)
library(tree)
library(plyr)
library(dplyr)
library(class)
library(rpart)
library(maptree)
library(ROCR)
library(reshape2)
```

```{r}
# Read in Data
spam <- read_table2("spambase.tab", guess_max=2000)
spam <- spam %>%
  mutate(y = factor(y, levels=c(0,1), labels=c("good", "spam"))) %>% # label as factors
  mutate_at(.vars=vars(-y), .funs=scale) 

dim(spam)
head(spam)
```

```{r}
# Error Rate Function
calc_error_rate <- function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}

# Keep Track of our model performance
records = matrix(NA, nrow=3, ncol=2)
colnames(records) <- c("train.error","test.error")
rownames(records) <- c("knn","tree","logistic")

# Train/Test split
set.seed(1)
test.indices = sample(1:nrow(spam), 1000)
spam.train=spam[-test.indices,]
spam.test=spam[test.indices,]

dim(spam.train)
dim(spam.test)

# Folds for CV
nfold = 10
set.seed(1)
folds = seq.int(nrow(spam.train)) %>% ## sequential obs ids
  cut(breaks = nfold, labels=FALSE) %>% ## sequential fold ids
  sample ## random fold ids
```

# K-Nearest Neighbor Method
## PART 1
```{r}
# CV
do.chunk <- function(chunkid, folddef, Xdat, Ydat, k){
  
  train = (folddef!=chunkid)
  
  Xtr = Xdat[train,]
  Ytr = Ydat[train]
  Xvl = Xdat[!train,]
  Yvl = Ydat[!train]
  
  ## get classifications for current training chunks
  predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, k = k)
  
  ## get classifications for current test chunk
  predYvl = knn(train = Xtr, test = Xvl, cl = Ytr, k = k)
  
  data.frame(fold = chunkid, # k folds
             train.error = calc_error_rate(predYtr, Ytr),
             val.error = calc_error_rate(predYvl, Yvl))
}
```

```{r}
# Check for missing values
sum(is.na(spam.train))
sum(is.na(spam.test))
```


```{r}
# Clean Up our train and test sets
Xtrain <- spam.train %>% select(-y) 
Ytrain <- spam.train$y

Xtest <- spam.test %>% select(-y) 
Ytest <- spam.test$y
```

```{r}
kvec = c(1, seq(10, 50, length.out=5))
kvec
```

```{r}
error.folds = NULL

set.seed(1)

for (j in kvec){
  tmp = ldply(1:nfold, do.chunk, # Apply do.chunk() function to each fold
              folddef=folds, Xdat=Xtrain, Ydat=Ytrain, k=j)# Necessary arguments to be passed into do.chunk
  tmp$neighbors = j # Keep track of each value of neighors
  error.folds = rbind(error.folds, tmp) # combine results
}
```

```{r}
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')

# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
  # Select all rows of validation errors
  filter(variable=='val.error') %>%
  # Group the selected data frame by neighbors
  group_by(neighbors, variable) %>%
  # Calculate CV error rate for each k
  summarise_each(funs(mean), error) %>%
  # Remove existing group
  ungroup() %>%
    filter(error==min(error))

val.error.means
```

```{r}
# Best # of neighbors
best.kfold = max(val.error.means$neighbors)
best.kfold
```

When k = 10, we get the smallest estimated test error.

## PART 2
```{r}
set.seed(1)

# Test
pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=best.kfold)

# Confusion matrix
conf.matrix = table(predicted=pred.YTest, true=Ytest)
conf.matrix

# Test accuracy rate
sum(diag(conf.matrix)/sum(conf.matrix))

# Test error rate
1 - sum(diag(conf.matrix)/sum(conf.matrix))
```

```{r}
# Plot errors
ggplot(errors, aes(x=neighbors, y=error, color=variable))+ 
    geom_line(aes(group=interaction(variable,fold))) +
    stat_summary(aes(group=variable), fun.y="mean", geom='line', size=3) +
    geom_vline(aes(xintercept=best.kfold), linetype='dashed')+
  ggtitle('Error Comparison')
```

```{r}
# Train Error
val.error.means[1,3]

# Test Error
calc_error_rate(pred.YTest, Ytest)

# Store Error in Records
records[1,1] <- as.numeric(unlist(val.error.means[1,3]))
records[1,2] <- calc_error_rate(pred.YTest, Ytest)
records
```

# Decision Trees
## PART 3
```{r}
set.seed(1)

# Set control
control <- tree.control(nrow(spam.train), minsize = 5, mindev = 1e-5)

# Create full tree
spamtree = tree(y ~., control = control, data = spam.train)
summary(spamtree)
```

We have 184 leaf nodes and there are 48 training observations out of 3601 that were misclassified.

## PART 4
```{r}
# Prune and draw tree
prune = prune.tree(spamtree, best = 10, method = 'misclass')
draw.tree(prune, nodeinfo = TRUE, cex = 0.5)
```

## PART 5
```{r}
set.seed(1)

# K-Fold cross validation
cv = cv.tree(spamtree, FUN=prune.misclass, K=nfold, rand = folds)

# Print out cv
cv
 
# Best size
best.size.cv = min(cv$size[cv$dev == min(cv$dev)])
best.size.cv
```

The optimal tree is size is 37. 

```{r}
# Plot Missclassification vs Tree Size
dtrees_df <- data.frame(cv$size, cv$dev)
ggplot(dtrees_df, aes(x=cv.size, y=cv.dev)) + 
  geom_point() +
  ggtitle('Tree Size and Misclassification') + 
  geom_vline(xintercept = best.size.cv, linetype='longdash', color = 'red') + 
  geom_text(aes(x=best.size.cv+5, label="Best Size CV", y=1000), colour="blue", angle=90, text=element_text(size=10))
```


## PART 6

```{r}
spamtree.pruned = prune.tree(spamtree, best = best.size.cv, method = 'misclass')

# Predict on test set
pred.pt.prune = predict(spamtree.pruned, Xtest, type="class")

# Obtain confusion matrix
err.pt.prune = table(pred.pt.prune, Ytest)
err.pt.prune

# Test error rate (Classification Error)
1-sum(diag(err.pt.prune))/sum(err.pt.prune)
```

```{r}
# Get test prediction
pred.pt.pruneTrain = predict(spamtree.pruned, Xtrain, type="class")

# Calculate Train and Test Error
dtree_test_error <- calc_error_rate(pred.pt.prune, Ytest)
dtree_train_error <- calc_error_rate(pred.pt.pruneTrain, Ytrain)

# Put in records
records[2,1] <- dtree_train_error
records[2,2] <- dtree_test_error
records
```

# Logistics Regression
## PART 7
### 7a

Given, $$p(z) = \frac{e^z} {1 +e^z} $$
$$p({1 +e^z}) = e^z$$
$$p + pe^z = e^z$$

$$pe^z = e^z - p$$
$$p = 1 - \frac{p}{e^z}$$
$$ 1 - p = \frac{p}{e^z}$$
$$e^z = \frac{p}{1-p}$$
$$z = ln(\frac{p}{1-p}) $$

### 7b


#$$p = \frac{e^{\beta_0+\beta_1 x_1}}{1+e^{\beta_0+\beta_1 x_1}}$$
When $x_1 = x_1 + 2$:

#$$p = \frac{e^{\beta_0+\beta_1 (x_1+2)}}{1+e^{\beta_0+\beta_1 (x_1+2)}} = \frac{e^{\beta_0+\beta_1 x_1+2 \beta_1}}{1+e^{\beta_0+\beta_1 x_1+2 \beta_1}} = \frac{e^{\beta_0+\beta_1 x_1 }e^{2 \beta_1}}{1+e^{\beta_0+\beta_1 x_1 }e^{2 \beta_1}}$$

As x increases by 2, the odds is multiplied by $e^{2\beta_1}$. 

#$$\lim_{x \to \infty} p = 0$$

Also, as x goes to infinity, the numerator becomes smaller and the denominator becomes bigger. Therefore, the probability gets closer to 0.

#$$\lim_{x \to -\infty} p = 1$$
As x goes to negative infinity, the probability goes to 1. 

## PART 8
```{r}
set.seed(1)

# Fit logistic regression
glm.fit = glm(y ~ ., data=spam.train, family=binomial)

# Summary
summary(glm.fit)
```


```{r}
# Test
trainP <- predict(glm.fit, type = "response")
testP <- predict(glm.fit, newdata = Xtest, type = "response")

# Save the predicted labels using 0.5 as a threshold
spam.train <- spam.train %>%
  mutate(predspam=as.factor(ifelse(trainP > 0.5, "spam", "good")))

spam.test <- spam.test %>%
  mutate(predspam=as.factor(ifelse(testP > 0.5, "spam", "good")))
```

```{r}
# Store errors in records
records[3,1] <- calc_error_rate(spam.train$predspam, Ytrain)
records[3,2] <- calc_error_rate(spam.test$predspam, Ytest)

# Compare errors
records
```

The method with the lowest classification error is decision trees, with a test error of 0.072.

# ROC
## PART 9
```{r}
# First arument is the prob.training, second is true labels
prob.tree.Test = predict(spamtree.pruned, Xtest, type="vector")
prob.glm.Test = predict(glm.fit, Xtest, type="response")

# ROC Set Up
pred.tree.ROC = prediction(prob.tree.Test[,2], Ytest)
pred.glm.ROC = prediction(prob.glm.Test, Ytest)

# We want TPR on the y axis and FPR on the x axis
perf.tree = performance(pred.tree.ROC, measure="tpr", x.measure="fpr")

# We want TPR on the y axis and FPR on the x axis
perf.glm = performance(pred.glm.ROC, measure="tpr", x.measure="fpr")

plot(perf.tree, col=3, lwd=3, main="ROC curve")
abline(0,1)
plot(perf.glm, add = TRUE, col = 2, lwd = 3, lty = 2)

legend("bottomright", legend=c("Decision Tree", "GLM"),
       col=c("green", "red"), lty=1:2)
```

```{r}
# Calculate AUC
auc.tree = performance(pred.tree.ROC, "auc")@y.values
auc.tree

# Calculate AUC
auc.glm = performance(pred.glm.ROC, "auc")@y.values
auc.glm
```

By using the Area Under the Curve metric, we can say that the Logistic Model is better because there is more area under the curve.

## PART 10
We are more concerned about false positive rates that are too large, because that means we are placing good emails into the spam folder (these could be important emails). It is important that good emails are not thrown in the spam folder. We are not worried about true positive rates being small because for most people, it is not extremely necessary to put every single spam in the spam folder. The user can always delete the spam in their regular inbox themselves, but they cannot easily recovery or notice if a good email is thrown into spam.